{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alda_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_GdtjUgiIEFd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import gc\n",
        "import string\n",
        "import unicodedata\n",
        "import operator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, roc_curve, cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras.layers import Activation, Wrapper\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import (Input, Embedding, SpatialDropout1D, Bidirectional, LSTM,\n",
        "                          CuDNNLSTM, Flatten, Concatenate, Dense, Conv1D, MaxPooling1D)\n",
        "from keras.initializers import glorot_normal, orthogonal\n",
        "from keras.models import Model\n",
        "from keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
        "                             ReduceLROnPlateau)\n",
        "\n",
        "from keras.layers import concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNLSTM, GRU, Dense, Bidirectional, SpatialDropout1D, Conv1D\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "MAX_FEATURES = int(2.5e5)\n",
        "MAX_LEN = 75\n",
        "NFOLDS = 5\n",
        "SEED = 1\n",
        "SPATIAL_DROPOUT = 0.24\n",
        "RNN_UNITS = 80\n",
        "MODEL_PATH = \"weights_best.hdf5\"\n",
        "CONST_LSTM = \"lstm\"\n",
        "CONST_GRU = \"gru\"\n",
        "CONST_CNN = \"cnn\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C7Fs6PrVWFEM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ]
    },
    {
      "metadata": {
        "id": "n1vBWn8cIEFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "\tdf_train = pd.read_csv(\"./data/train.csv\")\n",
        "\tdf_test = pd.read_csv(\"./data/test.csv\")\n",
        "\n",
        "\treturn df_train, df_test\n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NcdrT2qJIEFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train, df_test = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nu_dIAGLWCi-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ]
    },
    {
      "metadata": {
        "id": "alVlcMLsIEFw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(df_train.head())\n",
        "\n",
        "print(len(df_train[df_train['target'] == 1]))\n",
        "print(len(df_train[df_train['target'] == 0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hhIV8dlLIEF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapping = dict()\n",
        "def to_dict(x):\n",
        "    arr = x.split(\" \")\n",
        "    \n",
        "    for word in arr:\n",
        "        word = word.lower()\n",
        "        if word in mapping:\n",
        "            mapping[word] += 1\n",
        "        else:\n",
        "            mapping[word] = 1\n",
        "\n",
        "df_train['question_text'].apply(to_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "agR2lorUIEGB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mapping\n",
        "type(mapping.values())\n",
        "len(mapping.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whz558OPIEGO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "freq_map = pd.DataFrame.from_dict({'word': list(mapping.keys()), 'count': list(mapping.values()) })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_EVJnvWcIEGY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "freq_map.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nl5vUgqtIEG6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "freq_map = freq_map.sort_values(by = [\"count\"], ascending=False).reset_index(drop=True)\n",
        "top_250k = freq_map[:250000][\"count\"].values.sum()\n",
        "top_450k = freq_map[:450690][\"count\"].values.sum()\n",
        "float(top_250k)/top_450k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "61b2VAVfIEHG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "freq_map = freq_map.set_index('word')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lh5JHsDpIEHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "freq_map.loc[\"the\", \"count\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kx6zfeLZIEHR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sum(df_test['question_text'].isna())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XyZwbyjNIEHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_word_embedding(filepath):\n",
        "    \"\"\"\n",
        "    given a filepath to embeddings file, return a word to vec\n",
        "    dictionary, in other words, word_embedding\n",
        "    E.g. {'word': array([0.1, 0.2, ...])}\n",
        "    \"\"\"\n",
        "    def _get_vec(word, *arr):\n",
        "        return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "    print('load word embedding ......')\n",
        "    try:\n",
        "        word_embedding = dict(_get_vec(*w.split(' ')) for w in open(filepath))\n",
        "    except UnicodeDecodeError:\n",
        "        word_embedding = dict(_get_vec(*w.split(' ')) for w in open(\n",
        "            filepath, encoding=\"utf8\", errors='ignore'))\n",
        "    # sanity check word vector length\n",
        "    words_to_del = []\n",
        "    for word, vec in word_embedding.items():\n",
        "        if len(vec) != 300:\n",
        "            words_to_del.append(word)\n",
        "    for word in words_to_del:\n",
        "        del word_embedding[word]\n",
        "    return word_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tauLZznrIEHj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#download link : http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
        "word_embedding = load_word_embedding(\"./glove.42B.300d.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOzyMOFFV7Zd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "gJxA1J08IEHw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_misspell(text):\n",
        "    \"\"\"\n",
        "    misspell list (quora vs. glove)\n",
        "    \"\"\"\n",
        "    misspell_to_sub = {\n",
        "        'fortnite': 'video game ',\n",
        "        'Swachh': 'swachh bharat mission campaign ',\n",
        "        'Quorans': 'quoran',\n",
        "        'Qoura ': 'quora ',\n",
        "        'quoras': 'quora',\n",
        "        'Quroa': 'quora',\n",
        "        'QUORA': 'quora',\n",
        "        'qoura': 'quora',\n",
        "        'Fortnite': 'video game',\n",
        "        'redmi': 'mobile phone',\n",
        "        'oneplus': 'mobile phone',\n",
        "        '₹': 'rupee',\n",
        "        'upwork': 'job website',\n",
        "        'unacademy': 'education website',\n",
        "        'byju': 'education website',\n",
        "        'padmaavati': 'bollywood movie',\n",
        "        'padmaavat': 'bollywood movie',\n",
        "        'bahubali': 'bollywood movie',\n",
        "        'quorans': 'quoran'\n",
        "    }\n",
        "    \n",
        "    misspell_re = re.compile('(%s)' % '|'.join(misspell_to_sub.keys()))\n",
        "\n",
        "    def _replace(match):\n",
        "        \"\"\"\n",
        "        reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa\n",
        "        \"\"\"\n",
        "        try:\n",
        "            word = misspell_to_sub.get(match.group(0))\n",
        "        except KeyError:\n",
        "            word = match.group(0)\n",
        "            print('!!Error: Could Not Find Key: {}'.format(word))\n",
        "        return word\n",
        "    return misspell_re.sub(_replace, text)\n",
        "\n",
        "\n",
        "def spacing_misspell(text):\n",
        "    \"\"\"\n",
        "    'deadbody' -> 'dead body'\n",
        "    \"\"\"\n",
        "    misspell_list = [\n",
        "        '(F|f)uck',\n",
        "        'Trump',\n",
        "        '\\W(A|a)nti',\n",
        "        '(W|w)hy',\n",
        "        '(W|w)hat',\n",
        "        'How',\n",
        "        'care\\W',\n",
        "        '\\Wover',\n",
        "        'gender',\n",
        "        'people',\n",
        "    ]\n",
        "    misspell_re = re.compile('(%s)' % '|'.join(misspell_list))\n",
        "    return misspell_re.sub(r\" \\1 \", text)\n",
        "\n",
        "\n",
        "def clean_latex(text):\n",
        "    \"\"\"\n",
        "    convert r\"[math]\\vec{x} + \\vec{y}\" to English\n",
        "    \"\"\"\n",
        "    # edge case\n",
        "    text = re.sub(r'\\[math\\]', ' LaTex math ', text)\n",
        "    text = re.sub(r'\\[\\/math\\]', ' LaTex math ', text)\n",
        "    text = re.sub(r'\\\\', ' LaTex ', text)\n",
        "\n",
        "    pattern_to_sub = {\n",
        "        r'\\\\mathrm': ' LaTex math mode ',\n",
        "        r'\\\\mathbb': ' LaTex math mode ',\n",
        "        r'\\\\boxed': ' LaTex equation ',\n",
        "        r'\\\\begin': ' LaTex equation ',\n",
        "        r'\\\\end': ' LaTex equation ',\n",
        "        r'\\\\left': ' LaTex equation ',\n",
        "        r'\\\\right': ' LaTex equation ',\n",
        "        r'\\\\(over|under)brace': ' LaTex equation ',\n",
        "        r'\\\\text': ' LaTex equation ',\n",
        "        r'\\\\vec': ' vector ',\n",
        "        r'\\\\var': ' variable ',\n",
        "        r'\\\\theta': ' theta ',\n",
        "        r'\\\\mu': ' average ',\n",
        "        r'\\\\min': ' minimum ',\n",
        "        r'\\\\max': ' maximum ',\n",
        "        r'\\\\sum': ' + ',\n",
        "        r'\\\\times': ' * ',\n",
        "        r'\\\\cdot': ' * ',\n",
        "        r'\\\\hat': ' ^ ',\n",
        "        r'\\\\frac': ' / ',\n",
        "        r'\\\\div': ' / ',\n",
        "        r'\\\\sin': ' Sine ',\n",
        "        r'\\\\cos': ' Cosine ',\n",
        "        r'\\\\tan': ' Tangent ',\n",
        "        r'\\\\infty': ' infinity ',\n",
        "        r'\\\\int': ' integer ',\n",
        "        r'\\\\in': ' in ',\n",
        "    }\n",
        "    # post process for look up\n",
        "    pattern_dict = {k.strip('\\\\'): v for k, v in pattern_to_sub.items()}\n",
        "    # init re\n",
        "    patterns = pattern_to_sub.keys()\n",
        "    pattern_re = re.compile('(%s)' % '|'.join(patterns))\n",
        "\n",
        "    def _replace(match):\n",
        "        \"\"\"\n",
        "        reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa\n",
        "        \"\"\"\n",
        "        try:\n",
        "            word = pattern_dict.get(match.group(0).strip('\\\\'))\n",
        "        except KeyError:\n",
        "            word = match.group(0)\n",
        "            print('!!Error: Could Not Find Key: {}'.format(word))\n",
        "        return word\n",
        "    return pattern_re.sub(_replace, text)\n",
        "\n",
        "def decontracted(text):\n",
        "    \"\"\"\n",
        "    de-contract the contraction\n",
        "    \"\"\"\n",
        "    # specific\n",
        "    text = re.sub(r\"(W|w)on(\\'|\\’)t\", \"will not\", text)\n",
        "    text = re.sub(r\"(C|c)an(\\'|\\’)t\", \"can not\", text)\n",
        "    text = re.sub(r\"(Y|y)(\\'|\\’)all\", \"you all\", text)\n",
        "    text = re.sub(r\"(Y|y)a(\\'|\\’)ll\", \"you all\", text)\n",
        "\n",
        "    # general\n",
        "    text = re.sub(r\"(I|i)(\\'|\\’)m\", \"i am\", text)\n",
        "    text = re.sub(r\"(A|a)in(\\'|\\’)t\", \"is not\", text)\n",
        "    text = re.sub(r\"n(\\'|\\’)t\", \" not\", text)\n",
        "    text = re.sub(r\"(\\'|\\’)re\", \" are\", text)\n",
        "    text = re.sub(r\"(\\'|\\’)s\", \" is\", text)\n",
        "    text = re.sub(r\"(\\'|\\’)d\", \" would\", text)\n",
        "    text = re.sub(r\"(\\'|\\’)ll\", \" will\", text)\n",
        "    text = re.sub(r\"(\\'|\\’)t\", \" not\", text)\n",
        "    text = re.sub(r\"(\\'|\\’)ve\", \" have\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def spacing_punctuation(text):\n",
        "    \"\"\"\n",
        "    add space before and after punctuation and symbols\n",
        "    \"\"\"\n",
        "    regular_punct = list(string.punctuation)\n",
        "    extra_punct = [\n",
        "        ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '₹', '&',\n",
        "        '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
        "        '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
        "        '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
        "        '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
        "        '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
        "        '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
        "        'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
        "        '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
        "        '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
        "    all_punct = ''.join(sorted(list(set(regular_punct + extra_punct))))\n",
        "    re_tok = re.compile(f'([{all_punct}])')\n",
        "    return re_tok.sub(r' \\1 ', text)\n",
        "\n",
        "\n",
        "def spacing_digit(text):\n",
        "    \"\"\"\n",
        "    add space before and after digits\n",
        "    \"\"\"\n",
        "    re_tok = re.compile('([0-9])')\n",
        "    return re_tok.sub(r' \\1 ', text)\n",
        "\n",
        "\n",
        "def spacing_number(text):\n",
        "    \"\"\"\n",
        "    add space before and after numbers\n",
        "    \"\"\"\n",
        "    re_tok = re.compile('([0-9]{1,})')\n",
        "    return re_tok.sub(r' \\1 ', text)\n",
        "\n",
        "\n",
        "def remove_number(text):\n",
        "    \"\"\"\n",
        "    numbers are not toxic\n",
        "    \"\"\"\n",
        "    return re.sub('\\d', '#', text)\n",
        "\n",
        "\n",
        "def remove_space(text):\n",
        "    \"\"\"\n",
        "    remove extra spaces and ending space if any\n",
        "    \"\"\"\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = re.sub('\\s+$', '', text)\n",
        "    return text\n",
        "\n",
        "def lower_casing(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def tokenizeSentences(df):\n",
        "    COLUMNS = [\"text\",\"lemma\",\"pos\",\"tag\",\"dep\",\"shape\",\"is_alpha\",\"is_stop_word\"]\n",
        "    #Iterate through every word of every sentence\n",
        "    tokenizedWords = pd.DataFrame(columns=COLUMNS)\n",
        "    for sent in df[\"question_text\"].values.tolist():\n",
        "        doc = nlp(sent.lower()) #Tokenize using spacy\n",
        "        for token in doc:\n",
        "            #Add new row for each new word\n",
        "            temp = pd.DataFrame(index = [token.text],\n",
        "                                columns=COLUMNS,\n",
        "                                data=[[\n",
        "                                        token.text,\n",
        "                                        token.lemma_, \n",
        "                                        token.pos_, \n",
        "                                        token.tag_, \n",
        "                                        token.dep_, \n",
        "                                        token.shape_, \n",
        "                                        token.is_alpha, \n",
        "                                        token.is_stop\n",
        "                                        ]])\n",
        "            tokenizedWords = pd.concat([tokenizedWords,temp])\n",
        "    #only unique words will exist\n",
        "    tokenizedWords = tokenizedWords.drop_duplicates()\n",
        "    return tokenizedWords\n",
        "\n",
        "def preprocess(text, remove_num=True):\n",
        "    \n",
        "    # 1. de-contract\n",
        "    text = decontracted(text)\n",
        "    # 2. clean misspell\n",
        "    text = clean_misspell(text)\n",
        "    # 3. space misspell\n",
        "    text = spacing_misspell(text)\n",
        "    # 4. clean_latex\n",
        "    text = clean_latex(text)\n",
        "    # 5. space\n",
        "    text = spacing_punctuation(text)\n",
        "    # 6. handle number\n",
        "    text = spacing_digit(text)    \n",
        "    text = remove_number(text)\n",
        "    # 7. remove space\n",
        "    text = remove_space(text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8R-kTH8hIEH-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(df_text, max_features):\n",
        "    # preprocess\n",
        "    df_text = df_text.progress_apply(preprocess)\n",
        "#     df_text = df_text.progress_apply(lower_casing)\n",
        "#     df_text = df_text.progress_apply(preprocess)\n",
        "    # tokenizer\n",
        "    tokenizer = Tokenizer(\n",
        "        num_words=max_features,\n",
        "        filters='',\n",
        "        lower=True,\n",
        "        split=' ')\n",
        "    # fit to data\n",
        "    tokenizer.fit_on_texts(list(df_text))\n",
        "    # tokenize the texts into sequences\n",
        "    sequences = tokenizer.texts_to_sequences(df_text)\n",
        "    return sequences, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f4CdUhfnIEIB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = df_train.target\n",
        "# get split index\n",
        "train_test_cut = df_train.shape[0]\n",
        "# get all text\n",
        "df_text = pd.concat(\n",
        "    [df_train['question_text'], df_test['question_text']],\n",
        "    axis=0).reset_index(drop=True)\n",
        "sequences, tokenizer = tokenize(df_text, max_features=MAX_FEATURES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7tY7NK9fIEIF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = pad_sequences(sequences, maxlen=MAX_LEN, padding='pre', truncating='post')  # noqa\n",
        "X_train = X[:train_test_cut]\n",
        "X_test = X[train_test_cut:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bZcYQq44IEIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train[5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JnjyAPYLWK_e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "Nym3it44IEIV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "create weights for embedding layer where row is the word index\n",
        "and collumns are the embedding dense vector\n",
        "\"\"\"\n",
        "def create_embedding_weights(word_index, word_embedding,\n",
        "                             max_features, paragram=False):\n",
        "    print('create word embedding weights ......')\n",
        "    # get entire embedding matrix\n",
        "    mat_embedding = np.stack(word_embedding.values())\n",
        "    # get shape\n",
        "    a, b = min(max_features, len(word_index)), mat_embedding.shape[1]\n",
        "    print('embedding weights matrix with shape: ({}, {})'.format(a, b))\n",
        "    # init embedding weight matrix\n",
        "    embedding_mean, embedding_std = mat_embedding.mean(), mat_embedding.std()\n",
        "    embedding_weights = np.random.normal(embedding_mean, embedding_std, (a, b))\n",
        "    # mapping\n",
        "    not_in_embedding = 0\n",
        "    not_in_embedding_map = []\n",
        "    for word, idx in word_index.items():\n",
        "        if idx >= a:\n",
        "            continue\n",
        "        if paragram:\n",
        "            word_vec = word_embedding.get(word.lower(), None)\n",
        "        else:\n",
        "            word_vec = word_embedding.get(word, None)\n",
        "        if word_vec is not None:\n",
        "            embedding_weights[idx] = word_vec\n",
        "        else:\n",
        "            not_in_embedding +=1\n",
        "\n",
        "            count = 0\n",
        "            if word in freq_map.index:\n",
        "                count = freq_map.loc[word, \"count\"]\n",
        "                \n",
        "#             print(count)\n",
        "            not_in_embedding_map.append((word, count))\n",
        "            \n",
        "    print(\"total not found in embeddings\")\n",
        "    print(not_in_embedding)\n",
        "    sorted_by_second = sorted(not_in_embedding_map, key=lambda tup: tup[1], reverse=True)\n",
        "    print(sorted_by_second[:100])\n",
        "    \n",
        "    return embedding_weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0iRzx_uQIEIc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove_weights = create_embedding_weights(tokenizer.word_index, word_embedding, MAX_FEATURES, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pjSD4-2sIEIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_embedding['93']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "adN0H9wuVlaj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Layers"
      ]
    },
    {
      "metadata": {
        "id": "MlZn5UhOIEIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_callbacks():\n",
        "    earlystopping = EarlyStopping(monitor='val_loss',\n",
        "                                  min_delta=0.0001,\n",
        "                                  patience=2,\n",
        "                                  verbose=2,\n",
        "                                  mode='auto')\n",
        "    checkpoint = ModelCheckpoint(filepath=MODEL_PATH,\n",
        "                                 monitor='val_loss',\n",
        "                                 save_best_only=True,\n",
        "                                 mode='min',\n",
        "                                 verbose=2)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                  min_lr=0.0001,\n",
        "                                  factor=0.6,\n",
        "                                  patience=1,\n",
        "                                  verbose=2)\n",
        "    return [earlystopping, checkpoint, reduce_lr]\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "\n",
        "\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
        "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return x / scale\n",
        "\n",
        "\n",
        "class Capsule(Layer):\n",
        "\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_size=(9, 1), share_weights=True,\n",
        "                 activation='default', **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_size = kernel_size\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'default':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = Activation(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Capsule, self).build(input_shape)\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(1, input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),   # noqa\n",
        "                                     # shape=self.kernel_size,\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "        else:\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(input_num_capsule,\n",
        "                                            input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),   # noqa\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "\n",
        "    def call(self, u_vecs):\n",
        "        if self.share_weights:\n",
        "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
        "        else:\n",
        "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(u_vecs)[0]\n",
        "        input_num_capsule = K.shape(u_vecs)[1]\n",
        "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
        "                                            self.num_capsule, self.dim_capsule))    # noqa\n",
        "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]  # noqa\n",
        "\n",
        "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]  # noqa\n",
        "        for i in range(self.routings):\n",
        "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]    # noqa\n",
        "            c = K.softmax(b)\n",
        "            c = K.permute_dimensions(c, (0, 2, 1))\n",
        "            b = K.permute_dimensions(b, (0, 2, 1))\n",
        "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))    # noqa\n",
        "            if i < self.routings - 1:\n",
        "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)\n",
        "\n",
        "\n",
        "class DropConnect(Wrapper):\n",
        "\n",
        "    def __init__(self, layer, prob, **kwargs):\n",
        "        self.prob = prob\n",
        "        self.layer = layer\n",
        "        super(DropConnect, self).__init__(layer, **kwargs)\n",
        "        if 0. < self.prob < 1.:\n",
        "            self.uses_learning_phase = True\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not self.layer.built:\n",
        "            self.layer.build(input_shape)\n",
        "            self.layer.built = True\n",
        "        super(DropConnect, self).build()\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return self.layer.compute_output_shape(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        if 0. < self.prob < 1.:\n",
        "            self.layer.kernel = K.in_train_phase(\n",
        "                K.dropout(self.layer.kernel, self.prob),\n",
        "                self.layer.kernel)\n",
        "            self.layer.bias = K.in_train_phase(\n",
        "                K.dropout(self.layer.bias, self.prob),\n",
        "                self.layer.bias)\n",
        "        return self.layer.call(x)    \n",
        "    \n",
        "def get_embedding_layer(embed_weights=None):\n",
        "    input_dim = embed_weights.shape[0]\n",
        "    output_dim = embed_weights.shape[1]\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=input_dim,\n",
        "        output_dim=output_dim,\n",
        "        weights=[embed_weights],\n",
        "        trainable=False,\n",
        "        name='embedding'\n",
        "    )\n",
        "    del embed_weights, input_dim, output_dim\n",
        "    gc.collect()\n",
        "    return embedding_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qlz3dsXPR1G9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **CNN Model Implementation**"
      ]
    },
    {
      "metadata": {
        "id": "Gg6tIteGRE9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_conv_pool(x_input, sufix, n_grams=[3,4,5], feature_maps=100):\n",
        "    branches = []\n",
        "    for n in n_grams:\n",
        "        branch = Conv1D(filters=feature_maps, kernel_size=n, activation='relu', name='Conv_'+sufix+'_'+str(n))(x_input)\n",
        "        branch = MaxPooling1D(pool_size=2, strides=None, padding='valid', name='MaxPooling_'+sufix+'_'+str(n))(branch)\n",
        "        branch = Flatten(name='Flatten_'+sufix+'_'+str(n))(branch)\n",
        "        branches.append(branch)\n",
        "    return branches\n",
        "\n",
        "\n",
        "def get_cnn_model(embed_weights):\n",
        "    \n",
        "    i = Input(shape=(MAX_LEN,), dtype='int32', name='main_input')\n",
        "    embedding_layer = get_embedding_layer(embed_weights)\n",
        "\n",
        "    x = embedding_layer(i)\n",
        "\n",
        "    # generate several branches in the network, each for a different convolution+pooling operation,\n",
        "    # and concatenate the result of each branch into a single vector\n",
        "    branches = get_conv_pool(x, 'static')\n",
        "    z = concatenate(branches, axis=-1)\n",
        "\n",
        "    # pass the concatenated vector to the predition layer\n",
        "    o = Dense(1, activation='sigmoid', name='output')(z)\n",
        "\n",
        "    model = Model(inputs=i, outputs=o)\n",
        "    model.compile(loss={'output': 'binary_crossentropy'}, optimizer='adam')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFgSH-XiSLDg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTM Model Implementation"
      ]
    },
    {
      "metadata": {
        "id": "nkkf1rg8RF5B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_lstm_model(embed_weights):\n",
        "    input_layer = Input(shape=(MAX_LEN, ), name='input')\n",
        "    # 1. embedding layer\n",
        "    # get embedding weights    \n",
        "    embedding_layer = get_embedding_layer(embed_weights)\n",
        "    x = embedding_layer(input_layer)\n",
        "    # 2. dropout\n",
        "    x = SpatialDropout1D(rate=SPATIAL_DROPOUT)(x)\n",
        "    # 3. bidirectional lstm\n",
        "    x = Bidirectional(\n",
        "        layer=LSTM(RNN_UNITS, return_sequences=True,\n",
        "                        kernel_initializer=glorot_normal(seed=1029),\n",
        "                        recurrent_initializer=orthogonal(gain=1.0, seed=1029)),\n",
        "        name='bidirectional_lstm')(x)\n",
        "    # 4. capsule layer\n",
        "    capsul = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x) # noqa\n",
        "    capsul = Flatten()(capsul)\n",
        "    capsul = DropConnect(Dense(32, activation=\"relu\"), prob=0.01)(capsul)\n",
        "\n",
        "    # 5. attention later\n",
        "    atten = Attention(step_dim=MAX_LEN, name='attention')(x)\n",
        "    atten = DropConnect(Dense(16, activation=\"relu\"), prob=0.05)(atten)\n",
        "    x = Concatenate(axis=-1)([capsul, atten])\n",
        "\n",
        "    # 6. output (sigmoid)\n",
        "    output_layer = Dense(units=1, activation='sigmoid', name='output')(x)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    # compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ke0Q7AsZSTsX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GRU Model Implementation"
      ]
    },
    {
      "metadata": {
        "id": "BW3hlIr2RGOH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def get_gru_model(embed_weights):\n",
        "    input_layer = Input(shape=(MAX_LEN, ), name='input')\n",
        "    # 1. embedding layer\n",
        "    # get embedding weights    \n",
        "    embedding_layer = get_embedding_layer(embed_weights)\n",
        "    x = embedding_layer(input_layer)\n",
        "    # 2. dropout\n",
        "    x = SpatialDropout1D(rate=SPATIAL_DROPOUT)(x)\n",
        "    # 3. bidirectional lstm\n",
        "    x = Bidirectional(\n",
        "        layer=GRU(RNN_UNITS, return_sequences=True,\n",
        "                        kernel_initializer=glorot_normal(seed=1029),\n",
        "                        recurrent_initializer=orthogonal(gain=1.0, seed=1029)),\n",
        "        name='bidirectional_lstm')(x)\n",
        "    # 4. capsule layer\n",
        "    capsul = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x) # noqa\n",
        "    capsul = Flatten()(capsul)\n",
        "    capsul = DropConnect(Dense(32, activation=\"relu\"), prob=0.01)(capsul)\n",
        "\n",
        "    # 5. attention later\n",
        "    atten = Attention(step_dim=MAX_LEN, name='attention')(x)\n",
        "    atten = DropConnect(Dense(16, activation=\"relu\"), prob=0.05)(atten)\n",
        "    x = Concatenate(axis=-1)([capsul, atten])\n",
        "\n",
        "    # 6. output (sigmoid)\n",
        "    output_layer = Dense(units=1, activation='sigmoid', name='output')(x)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    # compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IZnnsXOJmWYD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model(embed_weights=None, model_name=CONST_LSTM):\n",
        "    if(model_name == CONST_CNN):\n",
        "        return get_cnn_model(embed_weights)\n",
        "    elif(model_name == CONST_GRU):\n",
        "        return get_gru_model(embed_weights)\n",
        "    else:\n",
        "        return get_lstm_model(embed_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aat8PRxUIEIs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed_weights = np.mean((glove_weights, glove_weights), axis=0)\n",
        "# print('embedding weights with shape: {}'.format(embed_weights.shape))\n",
        "# train models\n",
        "kfold = StratifiedKFold(n_splits=NFOLDS, random_state=SEED, shuffle=True)\n",
        "best_thres = []\n",
        "y_submit = np.zeros((X_test.shape[0], ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rAjpo0hhIEIx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train_1_idx = y_train[y_train == 1][:100].index\n",
        "y_train_0_idx = y_train[y_train == 0][:100].index\n",
        "\n",
        "\n",
        "X_train_1 = X_train[y_train_1_idx]\n",
        "X_train_0 = X_train[y_train_0_idx]\n",
        "X_train_sample = np.append(X_train_1, X_train_0, axis=0)\n",
        "X_train_sample\n",
        "y_train_sample = [1]*100 + [0]*100\n",
        "X_train_sample.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWPHEXz8StQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculating Performance Metrics"
      ]
    },
    {
      "metadata": {
        "id": "MDUNEuDuWsBa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "  f1 = f1_score(y_true, y_pred) \n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  pre = precision_score(y_true, y_pred)\n",
        "  rec = recall_score(y_true, y_pred)\n",
        "  roc = roc_curve(y_true, y_pred)\n",
        "  kap = cohen_kappa_score(y_true, y_pred)\n",
        "  \n",
        "  print(\"F1 score \" + str(f1))\n",
        "  print(\"Confusion metrics \" + str(cm))\n",
        "  print(\"Precision \" + str(pre))\n",
        "  print(\"Recall \" + str(rec))\n",
        "  print(\"ROC curve score \" + str(roc))\n",
        "  print(\"Kappa score \" + str(kap))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7JmQPQrcS8BW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42, stratify=y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cqRzk33dVZLh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Models"
      ]
    },
    {
      "metadata": {
        "id": "LIsdjLFTIEI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, (idx_train, idx_val) in enumerate(kfold.split(X_train, y_train)):\n",
        "    # data\n",
        "    X_t = X_train[idx_train]\n",
        "    y_t = y_train[idx_train]\n",
        "    X_v = X_train[idx_val]\n",
        "    y_v = y_train[idx_val]\n",
        "    model = get_model(embed_weights, CONST_CNN)\n",
        "    \n",
        "    if i == 0:\n",
        "        print(model.summary())\n",
        "    # get class weight\n",
        "    weights = None\n",
        "    weights = utils.class_weight.compute_class_weight('balanced', np.unique(y_t), y_t)    # noqa\n",
        "    # train\n",
        "    model.fit(\n",
        "        X_t, y_t,\n",
        "        batch_size=512, epochs=5,\n",
        "        validation_data=(X_v, y_v),\n",
        "        verbose=2, callbacks=get_callbacks(),\n",
        "        class_weight=weights)\n",
        "    # reload best model\n",
        "    model.load_weights(MODEL_PATH)\n",
        "    # get f1 threshold\n",
        "    y_proba = model.predict([X_v], batch_size=1024, verbose=2)\n",
        "    print(\"Metrics....\")\n",
        "    print(calculate_metrics(np.squeeze(y_v), (np.squeeze(y_proba) > 0.51).astype(int)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iP_Wp0PxVeG0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing models"
      ]
    },
    {
      "metadata": {
        "id": "rzV-q7s7PFTk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict([X_test], batch_size=1024, verbose=2)\n",
        "print(\"accurracy....\")\n",
        "print(calculate_metrics(np.squeeze(y_test), (np.squeeze(y_pred) > 0.51).astype(int)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7OihsooQQ_83",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train_1_idx = y_train[y_train == 1]\n",
        "y_train_0_idx = y_train[y_train == 0]\n",
        "print(\"Toxic count \" + str(len(y_train_1_idx)))\n",
        "print(\"Intoxic count \" + str(len(y_train_0_idx)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}